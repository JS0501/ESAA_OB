{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPD4j+R4+JT2jLkzMZI0WxQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JS0501/ESAA_OB/blob/main/ESAA1110.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.1 인공 신경망의 한계와 딥러닝 출현**"
      ],
      "metadata": {
        "id": "WX1w5UZIHUQd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "퍼셉트론은 신경망(딥러닝)의 기원이 되는 알고리즘이다.\n",
        "\n",
        "퍼셉트론은 다수의 신호(흐름이 있는)를 입력으로 받아 하나의 신호를 출력하는데, 이 신호를 입력으로 받아 '흐른다/안 흐른다(1 또는 0)'는 정보를 앞으로 전달하는 원리로 작동한다."
      ],
      "metadata": {
        "id": "o4MRk2gJHc_c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AND 게이트**\n",
        "\n",
        "AND 게이트는 모든 입력이 '1'일 때 작동한다.\n",
        "\n",
        "즉, 입력 중 어떤 하나라도 '0'을 갖는다면 작동을 멈춘다."
      ],
      "metadata": {
        "id": "hEg9Ujl8HqVd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OR 게이트**\n",
        "\n",
        "OR 게이트는 입력에서 둘 중 하나만 '1'이거나 둘 다 '1'일 때 작동한다.\n",
        "\n",
        "즉, 입력 모두가 '0'을 갖는 경우를 제외한 나머지가 모두 '1'값을 갖는다."
      ],
      "metadata": {
        "id": "7zJM76pwHyTJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**XOR 게이트**\n",
        "\n",
        "XOR 게이트는 배타적 논리합이라는 용어로 입력 두 개 중 한 개만 '1'일 때 작동하는 논리 연산이다.\n",
        "\n",
        "XOR 게이트는 데이터가 비선형적으로 분리되기 때문에 제대로 된 분류가 어렵다.\n",
        "\n",
        "즉, 단층 퍼셉트론에서는 AND, OR 연산에 대해서는 학습이 가능하지만, XOR에 대해서는 학습이 불가능하다."
      ],
      "metadata": {
        "id": "vUqWpzerH9J6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이를 극복하는 방안으로 입력층과 출력층 사이에 하나 이상의 중간층(은닉층)을 두어 비선형적으로 분리되는 데이터에 대해서도 학습이 가능하도록 다층 퍼셉트론을 고안했다.\n",
        "\n",
        "이때, 입력층과 출력층 사이에 은닉층이 여러 개 있는 신경망을 **심층 신경망**이라고 하며, 심층 신경망을 다른 이름으로 딥러닝이라고 한다."
      ],
      "metadata": {
        "id": "9bNkceUhILyK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.2 딥러닝 구조**"
      ],
      "metadata": {
        "id": "vEA2_7CIIbef"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4.2.1 딥러닝 용어**"
      ],
      "metadata": {
        "id": "rnwQuxl_Ido8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "딥러닝은 입력층, 출력층과 두 개 이상의 은닉층으로 구성되어 있다.\n",
        "\n",
        "또한 입력 신호를 전달하기 위해 다양한 함수를 사용한다."
      ],
      "metadata": {
        "id": "VqrLiuUEIffS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 가중치\n",
        "\n",
        "입력 값이 연산 결과에 미치는 영향력을 조절한다.\n",
        "\n",
        "- 가중합 또는 전달 함수\n",
        "\n",
        "각 노드에서 들어오는 신호에 가중치를 곱해서 다음 노드로 전달되는데, 이 값들을 모두 더한 합계를 가중합이라고 한다.\n",
        "\n",
        "또한, 노드의 가중합이 계산되면 이 가중합을 활성화 함수로 보내기 때문에 전달 함수라고도 한다.\n",
        "\n",
        "- 활성화 함수\n",
        "\n",
        "전달 함수에서 전달받은 값을 출력할 때 일정 기준에 따라 출력 값을 변화시키는 비선형 함수이다.\n",
        "\n",
        "시그모이드, 하이퍼볼릭 탄젠트, 렐루 함수 등이 있다.\n",
        "\n",
        "1) 시그모이드 함수\n",
        "\n",
        "선형 함수의 결과를 0~1 사이에서 비선형 형태로 변형한다.\n",
        "\n",
        "주로 로지스틱 회귀와 같은 분류 문제를 확률적으로 표현하는 데 사용된다.\n",
        "\n",
        "딥러닝 모델의 깊이가 깊어지면 기울기가 살아지는 '기울기 소멸 문제'가 발생하여 딥러닝 모델에서는 잘 사용하지 않는다.\n",
        "\n",
        "2) 하이퍼볼릭 탄젠트 함수\n",
        "\n",
        "선형 함수의 결과를 -1~1 사이에서 비선형 형태로 변형한다.\n",
        "\n",
        "시그모이드에서 결괏값의 평균이 0이 아닌 양수로 편향된 문제를 해결하는 데 사용했지만, 기울기 소멸 문제는 여전히 발생한다.\n",
        "\n",
        "3) 렐루 함수\n",
        "\n",
        "입력(x)이 음수일 때는 0을 출력하고, 양수일 때는 x를 출력한다.\n",
        "\n",
        "경사 하강법에 영향을 주지 않아 학습 속도가 빠르고, 기울기 소멸 문제가 발생하지 않는다.\n",
        "\n",
        "렐루 함수는 일반적으로 은닉층에서 사용된다.\n",
        "\n",
        "문제는 음수 값을 입력받으면 항상 0을 출력하기 때문에 학습 능력이 감소하는데, 이를 해결하려고 리키 렐루(Leaky ReLU) 함수 등을 사용한다.\n",
        "\n",
        "4) 리키 렐루 함수\n",
        "\n",
        "입력 값이 음수이면 0이 아닌 0.001처럼 매우 작은 수를 반환한다.\n",
        "\n",
        "이렇게 하면 입력 값이 수렴하는 구간이 제거되어 렐루 함수를 사용할 때 생기는 문제를 해결할 수 있다.\n",
        "\n",
        "5) 소프트맥스 함수\n",
        "\n",
        "입력 값을 0~1 사이에 출력되도록 정규화하여 출력 값들의 총합이 항상 1이 되도록 한다.\n",
        "\n",
        "보통 딥러닝에서 출력 노드의 활성화 함수로 많이 사용된다."
      ],
      "metadata": {
        "id": "hay98hvsInm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "aRq4HnMqj2Oc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "  def __init__(self,n_feature,n_hidden,n_output):\n",
        "    super(Net,self).__init__()\n",
        "    self.hidden = torch.nn.Linear(n_feature,n_hidden) # 은닉층\n",
        "    self.relu = torch.nn.ReLU(inplace=True)\n",
        "    self.out = torch.nn.Linear(n_hidden,n_output) # 출력층\n",
        "    self.softmax = torch.nn.Softmax(dim=n_output)\n",
        "  def forward(self,x):\n",
        "    x = self.hidden(x)\n",
        "    x = self.relu(x) # 은닉층을 위한 렐루 활성화 함수\n",
        "    x = self.out(x)\n",
        "    x = self.softmax(x) # 출력층을 위한 소프트맥스 활성화 함수\n",
        "    return x"
      ],
      "metadata": {
        "id": "drHCDcAYIbHx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**손실 함수**\n",
        "\n",
        "경사 하강법은 학습률과 손실 함수의 순간 기울기를 이용하여 가중치를 업데이트하는 방법이다.\n",
        "\n",
        "즉, 미분의 기울기를 이용하여 오차를 비교하고 최소화하는 방향으로 이동시키는 방법이다.\n",
        "\n",
        "이때 오차를 구하는 방법이 손실 함수이다.\n",
        "\n",
        "즉, 손실 함수는 학습을 통해 얻은 데이터의 추정치가 실제 데이터와 얼마나 차이가 나는지 평가하는 지표이다.\n",
        "\n",
        "MSE, Cross Entropy Error 등이 있다."
      ],
      "metadata": {
        "id": "O6tqIZ39KMEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**평균 제곱 오차**\n",
        "\n",
        "실제 값과 예측 값의 차이(error)를 제곱하여 평균 낸 것이 MSE이다.\n",
        "\n",
        "이 값이 작을수록 예측력이 좋다.\n",
        "\n",
        "회귀에서 손실 함수로 주로 사용된다."
      ],
      "metadata": {
        "id": "LisqM2FAKc_q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jobn_ch6HMwM"
      },
      "outputs": [],
      "source": [
        "#import torch\n",
        "\n",
        "#loss_fn = torch.nn.MSELoss(reduction='sum')\n",
        "#y_pred = model(x)\n",
        "#loss = loss_fn(y_pred,y)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**크로스 엔트로피 오차**\n",
        "\n",
        "분류 문제에서 원-핫 인코딩 했을 때 사용할 수 있는 오차 계산법이다.\n",
        "\n",
        "두 개의 확률 분포 차이를 이용하기에 시그모이드 영향을 덜 받는다.\n",
        "\n",
        "따라서 평균 제곱 오차보다 학습 속도가 빠르다."
      ],
      "metadata": {
        "id": "5Ynk30rAKpUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#loss = nn.CrossEntroptyLoss()\n",
        "#input = torch.randn(5,6,requires_grad=True) # torch.randn은 평균이 0이고 표준편차가 1인 가우시안 정규분포를 이용하여 숫자를 생성\n",
        "#target = torch.empty(3,dtype=torch.long).random_(5) # torch.empty는 dtype torch.float32의 랜덤한 값으로 채워진 텐서를 반환\n",
        "#output = loss(input,target)\n",
        "#output.backward()"
      ],
      "metadata": {
        "id": "HDYcOK_1KyhA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4.2.2 딥러닝 학습**"
      ],
      "metadata": {
        "id": "YOAw1VggLBlE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "크게 순전파와 역전파라는 두 단계로 진행된다."
      ],
      "metadata": {
        "id": "VpsXGzLwLEDP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**순전파**는 네트워크에 훈련 데이터가 들어올 때 발생하며, 데이터를 기반으로 예측 값을 계산하기 위해 전체 신경망을 교차해 지나간다.\n",
        "\n",
        "즉, 모든 뉴런이 이전 층의 뉴런에서 수신한 정보에 변환(가중합 및 활성화 함수)을 적용하여 다음 층(은닉층)의 뉴런으로 전송하는 방식이다.\n",
        "\n",
        "네트워크를 통해 입력 데이터를 전달하며, 데이터가 모든 층을 통과하고 모든 뉴런이 계산을 완료하면 그 예측 값은 최종 층(출력층)에 도달하게 된다.\n",
        "\n",
        "손실 함수로 네트워크의 예측 값과 실제 값의 차이(손실, 오차)를 추정한다.\n",
        "\n",
        "이때 손실 함수 비용은 '0'이 이상적이다.\n",
        "\n",
        "따라서 손실 함수 비용이 0에 가깝도록 하기 위해 모델이 훈련을 반복하며 가중치를 조정한다.\n",
        "\n",
        "손실이 계산되면 그 정보는 역으로 전파(출력층->은닉층->입력층)되기 때문에 **역전파**라고 한다."
      ],
      "metadata": {
        "id": "7wca87ZFLHwi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4.2.3 딥러닝의 문제점과 해결 방안**"
      ],
      "metadata": {
        "id": "304jelCTLslq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "딥러닝의 핵심은 활성화 함수가 적용된 여러 은닉층을 결합하여 비선형 영역을 표현하는 것이다.\n",
        "\n",
        "활성화 함수가 적용된 은닉층 개수가 많을수록 데이터 분류가 잘되고 있음을 볼 수 있다.\n",
        "\n",
        "하지만 은닉층이 많을수록 다음 세 가지 문제점이 생긴다."
      ],
      "metadata": {
        "id": "wbLmAYsRL09_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**과적합 문제 발생**\n",
        "\n",
        "과적합은 훈련 데이터에 대해 과하게 학습하여 실제 데이터에 대한 오차가 증가하는 현상이다.\n",
        "\n",
        "과적합을 해결하는 방법으로 드롭아웃(dropout)이 있다.\n",
        "\n",
        "신경망 모델이 과적합되는 것을 피하기 위한 방법으로, 학습 과정 중 임의로 일부 노드들을 학습에서 제외시킨다."
      ],
      "metadata": {
        "id": "QIExSCOHL-fj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DropoutModel(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(DropoutModel,self).__init__()\n",
        "    self.layer1 = torch.nn.Linear(784,1200)\n",
        "    self.dropout1 = torch.nn.Dropout(0.5) # 50%의 노드를 무작위로 선택하여 사용하지 않겠다는 의미\n",
        "    self.layer2 = torch.nn.Linear(1200,1200)\n",
        "    self.dropout2 = torch.nn.Dropout(0.5)\n",
        "    self.layer3 = torch.nn.Linear(1200,10)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = F.relu(self.layer1(x))\n",
        "    x = self.dropout1(x)\n",
        "    x = F.relu(self.layer2(x))\n",
        "    x = self.dropout2(x)\n",
        "    return self.layer3(x)"
      ],
      "metadata": {
        "id": "9mUeRliYLDmA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**기울기 소멸 문제 발생**\n",
        "\n",
        "기울기 소멸 문제는 은닉층이 많은 신경망에서 주로 발생하는데, 출력층에서 은닉층으로 전달되는 오차가 크게 줄어들어 학습이 되지 않는 현상이다.\n",
        "\n",
        "즉, 기울기가 소멸되기 때문에 학습되는 양이 '0'에 가까워져 학습이 더디게 진행되다 오차를 더 줄이지 못하고 그 상태로 수렴하는 현상이다.\n",
        "\n",
        "렐루 활성화 함수를 사용하여 해결한다."
      ],
      "metadata": {
        "id": "kP7htY3jMn_s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**성능이 나빠지는 문제 발생**\n",
        "\n",
        "경사 하강법은 손실 함수의 비용이 최소가 되는 지점을 찾을 때까지 기울기가 낮은 쪽으로 계속 이동시키는 과정을 반복하는데, 이때 성능이 나빠지는 문제가 발생한다.\n",
        "\n",
        "이러한 문제를 개선하고나 확률적 경사 하강법과 미니 배치 경사 하강법을 이용한다."
      ],
      "metadata": {
        "id": "5BQkFJGCM1kb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **배치 경사 하강법**\n",
        "\n",
        "전체 데이터셋에 대한 오류를 구한 후 기울기를 한 번만 계산하여 모델의 파라미터를 업데이트하는 방법이다.\n",
        "\n",
        "즉, 전체 훈련 데이터셋에 대해 가중치를 편미분한다.\n",
        "\n",
        "학습이 오래 걸리는 단점이 있다."
      ],
      "metadata": {
        "id": "D2JK40VfNBVo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **확률적 경사 하강법**\n",
        "\n",
        "임의로 선택한 데이터에 대해 기울기를 계산하는 방법으로 적은 데이터를 사용하므로 빠른 계산이 가능하다.\n",
        "\n",
        "파라미터 변동 폭이 불안정하고, 때로는 배치 경사 하강법보다 정확도가 낮을 수 있지만 속도가 빠르다는 장점이 있다."
      ],
      "metadata": {
        "id": "cFce-UOiNK-I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **미니 배치 경사 하강법**\n",
        "\n",
        "전체 데이터셋을 미니 배치 여러 개로 나누고, 미니 배치 한 개마다 기울기를 구한 후 그것의 평균 기울기를 이용하여 모델을 업데이트해서 학습하는 방법이다.\n",
        "\n",
        "전체 데이터를 계산하는 것보다 빠르며, 확률적 경사 하강법보다 안정적이라는 장점이 있기에 실제로 가장 많이 사용한다."
      ],
      "metadata": {
        "id": "s71AchHzNWSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#class CustomDataset(Dataset):\n",
        "#  def __init__(self):\n",
        "#    self.x_data = [[1,2,3],[4,5,6],[7,8,9]]\n",
        "#    self.y_data = [[12],[18],[11]]\n",
        "#    def __len__(self):\n",
        "#      return len(self.x_data)\n",
        "#    def __getitem__(self,idx):\n",
        "#      x = torch.FloatTensor(self.x_data[idx])\n",
        "#      y = torch.FloatTensor(self.y_data[idx])\n",
        "#      return x,y\n",
        "#dataset = CustomDataset()\n",
        "#dataloader = DataLoader(\n",
        "#    dataset, # 데이터셋\n",
        "#    batch_size=2, # 미니 배치 크기로 2의 제곱수를 사용하겠다는 의미입니다.\n",
        "#    shuffle=True, # 데이터를 불러올 때마다 랜덤으로 섞어서 가져옵니다.\n",
        "#)"
      ],
      "metadata": {
        "id": "e8fN6tnLM1KT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**옵티마이저**\n",
        "\n",
        "확률적 경사 하강법의 파라미터 변경 폭이 불안정한 문제를 해결하기 위해 학습 속도와 운동량을 조정하는 옵티마이저(optimizer)를 적용해 볼 수 있다."
      ],
      "metadata": {
        "id": "-8UYIbNHN99l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 아다그라드 구현\n",
        "# optimizer = torch.optim.Adagrad(model.parameters(),lr=0.01)\n",
        "\n",
        "# 아다델타 구현\n",
        "# optimizer = torch.optim.Adadelta(model.parameters(),lr=1.0)\n",
        "\n",
        "# 알엠에스프롭 구현\n",
        "# optimizer = torch.optim.RMSprop(model.parameters(),lr=0.01)\n",
        "\n",
        "# 모멘텀 구현\n",
        "# optimizer = torch.optim.SGD(model.parameters(),lr=0.01,momentum=0.9)\n",
        "\n",
        "# 네스테로프 모멘텀 구현\n",
        "# optimizer = torch.optim.SGD(model.parameters(),lr=0.01,momentum=0.9,nesterov=True)\n",
        "\n",
        "# 아담 구현\n",
        "# optimizer = torch.optim.Adam(model.parameters(),lr=0.01)"
      ],
      "metadata": {
        "id": "crx32LefkbJF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4.2.4 딥러닝을 사용할 때 이점**"
      ],
      "metadata": {
        "id": "IsO5ZZjSOGrC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**특성 추출**\n",
        "\n",
        "딥러닝은 특성 추출 과정을 알고리즘에 통합시켰다.\n",
        "\n",
        "데이터 특성을 잘 잡아내고자 은닉층을 깊게 쌓는 방식으로 파라미터를 늘린 모델 구조 덕분이다.\n",
        "\n",
        "**빅데이터의 효율적 활용**\n",
        "\n",
        "확보된 데이터가 적다면 딥러닝의 성능 향상을 기대하기 힘들기 때문에 머신 러닝을 고려해 보아야 한다."
      ],
      "metadata": {
        "id": "RHxR5EEvOJnN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.3 딥러닝 알고리즘**"
      ],
      "metadata": {
        "id": "gbQU1yo1OYtf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "심층 신경망을 사용한다는 공통점이 있다."
      ],
      "metadata": {
        "id": "hkZLY75vObcr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4.3.1 심층 신경망**\n",
        "\n",
        "입력층과 출력층 사이에 다수의 은닉층을 포함하는 인공 신경망이다.\n",
        "\n",
        "다수의 은닉층을 두었기에 다양한 비선형적 관계를 학습할 수 있는 장점이 있지만, 학습을 위한 연산량이 많고 기울기 소멸 문제 등이 발생할 수 있다.\n",
        "\n",
        "이러한 문제 해결을 위해 드롭아웃, 렐루 함수, 배치 정규화 등을 적용해야 한다."
      ],
      "metadata": {
        "id": "r5DY27rkOdSQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4.3.2 합성곱 신경망**\n",
        "\n",
        "합성곱층(convolutional layer)과 풀링층(pooling layer)을 포함하는 이미지 처리 성능이 좋은 인공 신경망 알고리즘이다.\n",
        "\n",
        "영상 및 사진이 포함된 이미지 데이터에서 객체를 탐색하거나 객체 위치를 찾아내는 데 유용한 신경망이다."
      ],
      "metadata": {
        "id": "BInwI58qOreV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4.3.3 순환 신경망**\n",
        "\n",
        "시계열 데이터(음악, 영상 등) 같은 시간 흐름에 따라 변화하는 데이터를 학습하기 위한 인공 신경망이다.\n",
        "\n",
        "'순환'은 자기 자신을 참조한다는 것으로, 현재 결과가 이전 결과가 연관이 있다는 의미이다.\n",
        "\n",
        "순환 신경망은 기울기 소멸 문제로 학습이 제대로 되지 않는 문제가 있다.\n",
        "\n",
        "이를 해결하고자 메모리 개념을 도입한 LSTM(Long-Short Term Memory)이 순환 신경망에서 많이 사용된다."
      ],
      "metadata": {
        "id": "RbtgiqduO2o5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4.3.4 제한된 볼츠만 머신**\n",
        "\n",
        "볼츠만 머신은 가시층과 은닉층으로 구성된 모델이다.\n",
        "\n",
        "가시층은 은닉층과만 연결된다. (가시층과 가시층, 은닉층과 은닉층 사이에 연결은 없다.)"
      ],
      "metadata": {
        "id": "msEA_1wyPH7E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4.3.5 심층 신뢰 신경망**\n",
        "\n",
        "입력층과 은닉층으로 구성된 제한된 볼츠만 머신을 블록처럼 여러 층으로 쌓은 형태로 연결된 신경망이다.\n",
        "\n",
        "즉, 사전 훈련된 제한된 볼츠만 머신을 층층이 쌓아 올린 구조로, 레이블이 없는 데이터에 대한 비지도 학습이 가능하다.\n",
        "\n",
        "부분적인 이미지에서 전체를 연상하는 일반화와 추상화 과정을 구현할 때 사용하면 유용하다."
      ],
      "metadata": {
        "id": "1UCAf6iTPTRX"
      }
    }
  ]
}