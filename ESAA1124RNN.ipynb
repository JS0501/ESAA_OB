{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4hOv4Aq4DsPHplAz3Ti/e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JS0501/ESAA_OB/blob/main/ESAA1124RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://wikidocs.net/64703"
      ],
      "metadata": {
        "id": "KBh9PvFOycFA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **문자 단위 RNN(Char RNN): 실습 2개**"
      ],
      "metadata": {
        "id": "tohQEaFRyHQq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNN의 입출력의 단위가 단어 레벨이 아니라 문자 레벨(character-level)로 하여 RNN을 구현한다면, 이를 문자 단위 RNN이라고 한다.\n",
        "\n",
        "RNN 구조 자체가 달라진 것은 아니고, 입/출력의 단위가 문자로 바뀌었을 뿐이다.\n",
        "\n",
        "문자 단위 RNN을 다대다 구조로 구현해본다."
      ],
      "metadata": {
        "id": "Lo4wAVmzyNA1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. 문자 단위 RNN(Char RNN)**"
      ],
      "metadata": {
        "id": "Z_Y2g5n_yY10"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Rfr_qLv4xrX0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1) 훈련 데이터 전처리하기**"
      ],
      "metadata": {
        "id": "rCKyUw9pygqa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNN의 동작을 이해하기 위해 문자 시퀀스 apple을 입력받으면 pple!를 출력하는 RNN을 구현해본다.\n",
        "\n",
        "입력 데이터와 레이블 데이터에 대해서 문자 집합(vocabulary)을 만든다.\n",
        "\n",
        "여기서 문자 집합은 중복을 제거한 문자들의 집합이다."
      ],
      "metadata": {
        "id": "6KX-Olyzyj-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_str = 'apple'\n",
        "label_str = 'pple!'\n",
        "char_vocab = sorted(list(set(input_str+label_str)))\n",
        "vocab_size = len(char_vocab)\n",
        "print ('문자 집합의 크기 : {}'.format(vocab_size))"
      ],
      "metadata": {
        "id": "mvNRoSPHyd4i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4f7aa85-d64f-4751-d6e0-ee44e90b688a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문자 집합의 크기 : 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "현재 문자 집합에는 총 5개의 문자 !,a,e,l,p가 있다.\n",
        "\n",
        "이제 하이퍼 파라미터를 정의한다.\n",
        "\n",
        "이때 입력은 원-핫 벡터를 사용할 것이기에 입력의 크기는 문자 집합의 크기여야 한다."
      ],
      "metadata": {
        "id": "Qk6yTPp2yxyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = vocab_size # 입력의 크기는 문자 집합의 크기\n",
        "hidden_size = 5\n",
        "output_size = 5\n",
        "learning_rate = 0.1"
      ],
      "metadata": {
        "id": "cjQyxrfky56c"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "문자 집합에 고유한 정수를 부여한다."
      ],
      "metadata": {
        "id": "Y52DubsSy7V1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "char_to_index = dict((c, i) for i, c in enumerate(char_vocab)) # 문자에 고유한 정수 인덱스 부여\n",
        "print(char_to_index)"
      ],
      "metadata": {
        "id": "dP35ET1ty89i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cd87eea-5c51-42b0-e7f9-2b80129bfb07"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'!': 0, 'a': 1, 'e': 2, 'l': 3, 'p': 4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "!은 0, a는 1, e는 2, l은 3, p는 4가 부여되었다.\n",
        "\n",
        "나중에 예측 결과를 다시 문자 시퀀스로 보기 위해 반대로 정수로부터 문자를 얻을 수 있는 index_to_char을 만든다."
      ],
      "metadata": {
        "id": "lGXbXrEAy_ye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_char={}\n",
        "for key, value in char_to_index.items():\n",
        "    index_to_char[value] = key\n",
        "print(index_to_char)"
      ],
      "metadata": {
        "id": "mAu6HzuezEqZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bdfde6d-1167-46b8-a064-e09be122075e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: '!', 1: 'a', 2: 'e', 3: 'l', 4: 'p'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 입력 데이터와 레이블 데이터의 각 문자들을 정수로 맵핑한다."
      ],
      "metadata": {
        "id": "1F915LcCzGUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_data = [char_to_index[c] for c in input_str]\n",
        "y_data = [char_to_index[c] for c in label_str]\n",
        "print(x_data)\n",
        "print(y_data)"
      ],
      "metadata": {
        "id": "AecOlzZkzIO_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d4841f7-2963-4f59-f28b-e0d2c3333d6d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 4, 4, 3, 2]\n",
            "[4, 4, 3, 2, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "파이토치의 nn.RNN()은 기본적으로 3차원 텐서를 입력 받는다.\n",
        "\n",
        "그렇기에 배치 차원을 추가한다."
      ],
      "metadata": {
        "id": "x67T-LPwzJmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 배치 차원 추가\n",
        "# 텐서 연산인 unsqueeze(0)를 통해 해결할 수도 있었음.\n",
        "x_data = [x_data]\n",
        "y_data = [y_data]\n",
        "print(x_data)\n",
        "print(y_data)"
      ],
      "metadata": {
        "id": "sR4_aQqnzQfp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3f7a011-777d-4ffe-c3b2-b571035e9c95"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 4, 4, 3, 2]]\n",
            "[[4, 4, 3, 2, 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "입력 시퀀스의 각 문자들을 원-핫 벡터로 바꾼다."
      ],
      "metadata": {
        "id": "397rTMJjzSNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_one_hot = [np.eye(vocab_size)[x] for x in x_data]\n",
        "print(x_one_hot)"
      ],
      "metadata": {
        "id": "YWkVPCpYzUZ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61f67a19-dc5b-4f3f-832c-df55fe63669c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[0., 1., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 1.],\n",
            "       [0., 0., 0., 0., 1.],\n",
            "       [0., 0., 0., 1., 0.],\n",
            "       [0., 0., 1., 0., 0.]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "입력 데이터와 레이블 데이터를 텐서로 바꾼다."
      ],
      "metadata": {
        "id": "diuXGqD_zV-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)"
      ],
      "metadata": {
        "id": "v-_hlq7kzXLt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9063ef3-99fc-48ff-e90f-d45968e9f407"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2348034151.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
            "  X = torch.FloatTensor(x_one_hot)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 각 텐서의 크기를 확인한다."
      ],
      "metadata": {
        "id": "0u7oWY0PzYWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
        "print('레이블의 크기 : {}'.format(Y.shape))"
      ],
      "metadata": {
        "id": "kZbR-EnnzZdz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f4d2356-faa8-4ae8-ca3c-08bcb1ac650f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 데이터의 크기 : torch.Size([1, 5, 5])\n",
            "레이블의 크기 : torch.Size([1, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2) 모델 구현하기**"
      ],
      "metadata": {
        "id": "04gRV-17zcvY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNN 모델을 구현한다.\n",
        "\n",
        "fc는 완전 연결층(fully-connected layer)을 의미하며 출력층으로 사용된다."
      ],
      "metadata": {
        "id": "2k55TaaHzfsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(Net, self).__init__()\n",
        "        self.rnn = torch.nn.RNN(input_size, hidden_size, batch_first=True) # RNN 셀 구현\n",
        "        self.fc = torch.nn.Linear(hidden_size, output_size, bias=True) # 출력층 구현\n",
        "\n",
        "    def forward(self, x): # 구현한 RNN 셀과 출력층을 연결\n",
        "        x, _status = self.rnn(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "40xAQHXqzd6o"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "클래스로 정의한 모델을 net에 저장한다."
      ],
      "metadata": {
        "id": "JkSLW8BWzmdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net(input_size, hidden_size, output_size)"
      ],
      "metadata": {
        "id": "KV6jIHmQzn7r"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "입력된 모델에 입력을 넣어서 출력의 크기를 확인한다."
      ],
      "metadata": {
        "id": "X7eZOJROzpeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = net(X)\n",
        "print(outputs.shape) # 3차원 텐서"
      ],
      "metadata": {
        "id": "pmduhpPxzrRH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53628582-7539-44d8-aa0e-6e100305c462"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 5, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1,5,5)의 크기를 가지는데 각각 배치 차원, 시점(timesteps), 출력의 크기이다.\n",
        "\n",
        "나중에 정확도를 측정할 때는 이를 모두 펼쳐서 계산하게 되는데, 이때는 view를 이용하여 배치 차원과 시점을 하나로 만든다."
      ],
      "metadata": {
        "id": "R5GTXkmPzs5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs.view(-1, input_size).shape) # 2차원 텐서로 변환"
      ],
      "metadata": {
        "id": "0mquSXrFz0sO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b141409-4f47-4149-94d8-837280dc8a45"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "차원이 (5,5)가 되었다.\n",
        "\n",
        "이제 레이블 데이터의 크기를 확인한다."
      ],
      "metadata": {
        "id": "ndDlYWU5z2aQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y.shape)\n",
        "print(Y.view(-1).shape)"
      ],
      "metadata": {
        "id": "YNyG9gNhz6zR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06cb3fdd-e2e8-4127-ae8b-ec9e0ddd789a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 5])\n",
            "torch.Size([5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "레이블 데이터는 (1,5)의 크기를 가지는데, 마찬가지로 나중에 정확도를 측정할 때는 이걸 펼쳐서 계산한다.\n",
        "\n",
        "이 경우 (5)의 크기를 가지게 된다.\n",
        "\n",
        "이제 옵티마이저와 손실 함수를 정의한다."
      ],
      "metadata": {
        "id": "Jyle1piW0CgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), learning_rate)"
      ],
      "metadata": {
        "id": "J2vycsTK0I3m"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "총 100번의 에포크를 학습한다."
      ],
      "metadata": {
        "id": "M7BTlpjp0KTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(X)\n",
        "    loss = criterion(outputs.view(-1, input_size), Y.view(-1)) # view를 하는 이유는 Batch 차원 제거를 위해\n",
        "    loss.backward() # 기울기 계산\n",
        "    optimizer.step() # 아까 optimizer 선언 시 넣어둔 파라미터 업데이트\n",
        "\n",
        "    # 아래 세 줄은 모델이 실제 어떻게 예측했는지를 확인하기 위한 코드.\n",
        "    result = outputs.data.numpy().argmax(axis=2) # 최종 예측값인 각 time-step 별 5차원 벡터에 대해서 가장 높은 값의 인덱스를 선택\n",
        "    result_str = ''.join([index_to_char[c] for c in np.squeeze(result)])\n",
        "    print(i, \"loss: \", loss.item(), \"prediction: \", result, \"true Y: \", y_data, \"prediction str: \", result_str)"
      ],
      "metadata": {
        "id": "z8gAPaLy0OHt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8484567-595e-4364-bdba-306d6015ee1e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 loss:  1.7246029376983643 prediction:  [[2 2 1 2 2]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  eeaee\n",
            "1 loss:  1.4399950504302979 prediction:  [[4 4 4 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppe!\n",
            "2 loss:  1.2307071685791016 prediction:  [[4 4 4 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppe!\n",
            "3 loss:  1.0071935653686523 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "4 loss:  0.7995654344558716 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "5 loss:  0.6232446432113647 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "6 loss:  0.4799967408180237 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "7 loss:  0.3606454133987427 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "8 loss:  0.26281413435935974 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "9 loss:  0.19166454672813416 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "10 loss:  0.14191214740276337 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "11 loss:  0.10618631541728973 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "12 loss:  0.07987689971923828 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "13 loss:  0.06044841930270195 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "14 loss:  0.04641057178378105 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "15 loss:  0.03656803071498871 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "16 loss:  0.029558325186371803 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "17 loss:  0.0242668054997921 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "18 loss:  0.0202103890478611 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "19 loss:  0.017111295834183693 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "20 loss:  0.014661973342299461 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "21 loss:  0.012625378556549549 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "22 loss:  0.010883113369345665 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "23 loss:  0.009403839707374573 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "24 loss:  0.008195090107619762 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "25 loss:  0.007257201708853245 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "26 loss:  0.006545910146087408 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "27 loss:  0.005964414682239294 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "28 loss:  0.0054221912287175655 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "29 loss:  0.004911898635327816 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "30 loss:  0.004475883673876524 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "31 loss:  0.0041289967484772205 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "32 loss:  0.0038529695011675358 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "33 loss:  0.0036215968430042267 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "34 loss:  0.003415841143578291 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "35 loss:  0.0032255600672215223 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "36 loss:  0.0030477233231067657 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "37 loss:  0.002883523004129529 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "38 loss:  0.0027359360828995705 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "39 loss:  0.002607489237561822 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "40 loss:  0.0024985733907669783 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "41 loss:  0.0024064432363957167 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "42 loss:  0.002325455890968442 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "43 loss:  0.002249607350677252 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "44 loss:  0.0021753828041255474 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "45 loss:  0.002103277714923024 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "46 loss:  0.0020358541514724493 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "47 loss:  0.001975270686671138 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "48 loss:  0.0019220743561163545 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "49 loss:  0.0018749572336673737 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "50 loss:  0.0018323041731491685 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "51 loss:  0.001792451599612832 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "52 loss:  0.001754330238327384 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "53 loss:  0.001717465347610414 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "54 loss:  0.0016818810254335403 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "55 loss:  0.0016479823971167207 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "56 loss:  0.0016161032253876328 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "57 loss:  0.001586530008353293 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "58 loss:  0.0015593344578519464 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "59 loss:  0.0015341847902163863 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "60 loss:  0.0015107005601748824 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "61 loss:  0.0014881689567118883 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "62 loss:  0.0014661853201687336 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "63 loss:  0.0014447495341300964 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "64 loss:  0.0014238138683140278 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "65 loss:  0.0014037821674719453 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "66 loss:  0.0013846304500475526 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "67 loss:  0.0013665484730154276 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "68 loss:  0.001349393860436976 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "69 loss:  0.0013329043285921216 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "70 loss:  0.0013170323800295591 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "71 loss:  0.0013015881413593888 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "72 loss:  0.001286523649469018 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "73 loss:  0.0012717200443148613 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "74 loss:  0.001257320516742766 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "75 loss:  0.0012433010851964355 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "76 loss:  0.0012296860804781318 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "77 loss:  0.0012165465159341693 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "78 loss:  0.0012037402484565973 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "79 loss:  0.0011913382913917303 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "80 loss:  0.0011792697478085756 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "81 loss:  0.001167439273558557 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "82 loss:  0.001155775273218751 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "83 loss:  0.0011443255934864283 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "84 loss:  0.001133018871769309 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "85 loss:  0.001122021465562284 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "86 loss:  0.0011112142819911242 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "87 loss:  0.001100692548789084 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "88 loss:  0.0010903372894972563 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "89 loss:  0.0010801720200106502 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "90 loss:  0.0010701732244342566 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "91 loss:  0.0010603645350784063 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "92 loss:  0.0010507224360480905 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "93 loss:  0.001041222712956369 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "94 loss:  0.00103181810118258 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "95 loss:  0.0010225323494523764 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "96 loss:  0.0010134369367733598 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "97 loss:  0.0010044602677226067 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "98 loss:  0.0009956264402717352 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "99 loss:  0.0009869590867310762 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 더 많은 데이터로 문자 단위 RNN을 구현한다."
      ],
      "metadata": {
        "id": "hfy3GQ0H0QIo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. 더 많은 데이터로 학습한 문자 단위 RNN(Char RNN)**"
      ],
      "metadata": {
        "id": "VqEzgUMj0Ti8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "xpoy5iBU0Rti"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "다음과 같이 임의의 샘플을 만든다."
      ],
      "metadata": {
        "id": "tP7PGD6p0aPJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1) 훈련 데이터 전처리하기**"
      ],
      "metadata": {
        "id": "c8qee4Ag0cbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
        "            \"collect wood and don't assign them tasks and work, but rather \"\n",
        "            \"teach them to long for the endless immensity of the sea.\")"
      ],
      "metadata": {
        "id": "HDJRDhmD0b0B"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "문자 집합을 생성하고, 각 문자에 고유한 정수를 부여한다."
      ],
      "metadata": {
        "id": "CMT5pTdo0f2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "char_set = list(set(sentence)) # 중복을 제거한 문자 집합 생성\n",
        "char_dic = {c: i for i, c in enumerate(char_set)} # 각 문자에 정수 인코딩"
      ],
      "metadata": {
        "id": "MGD57gv_0h2y"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(char_dic) # 공백도 여기서는 하나의 원소"
      ],
      "metadata": {
        "id": "2gQaxXsq0jSM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04da600f-2c83-4c0b-ba5f-9c6f7c12c88f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{' ': 0, 'a': 1, ',': 2, 'o': 3, 'i': 4, '.': 5, 'n': 6, 'c': 7, 'b': 8, 'e': 9, 'k': 10, 'r': 11, 'm': 12, 'f': 13, 'u': 14, 'l': 15, 'h': 16, 'y': 17, 'g': 18, 's': 19, 'w': 20, 'd': 21, \"'\": 22, 't': 23, 'p': 24}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "각 문자에 정수가 부여되었으며, 총 25개의 문자가 존재한다.\n",
        "\n",
        "문자 집합의 크기를 확인한다."
      ],
      "metadata": {
        "id": "KJhu-Ecy0kne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dic_size = len(char_dic)\n",
        "print('문자 집합의 크기 : {}'.format(dic_size))"
      ],
      "metadata": {
        "id": "UnElCc9B0nd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e1b96fc-3950-4bd3-9411-55f2e409a7c9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문자 집합의 크기 : 25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "문자 집합의 크기는 25이며, 입력을 원-핫 벡터로 사용할 것이므로 이는 매 시점마다 들어갈 입력의 크기이기도 하다.\n",
        "\n",
        "이제 하이퍼 파라미터를 설정한다.\n",
        "\n",
        "hidden_size(은닉 상태의 크기)를 입력의 크기와 동일하게 줬는데, 이는 사용자의 선택으로 다른 값을 줘도 무방하다.\n",
        "\n",
        "sequence_length라는 변수를 선언했는데, 앞서 만든 샘플을 10개 단위로 끊어서 샘플을 만들 예정이기 때문이다."
      ],
      "metadata": {
        "id": "RKnTQEji0osW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼파라미터 설정\n",
        "hidden_size = dic_size\n",
        "sequence_length = 10  # 임의 숫자 지정\n",
        "learning_rate = 0.1"
      ],
      "metadata": {
        "id": "k4NnbJUs03BT"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "임의로 지정한 sequence_length 값인 10의 단위로 샘플들을 잘라서 데이터를 만든다."
      ],
      "metadata": {
        "id": "Ab_6zyr105JD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 구성\n",
        "x_data = []\n",
        "y_data = []\n",
        "\n",
        "for i in range(0, len(sentence) - sequence_length):\n",
        "    x_str = sentence[i:i + sequence_length]\n",
        "    y_str = sentence[i + 1: i + sequence_length + 1]\n",
        "    print(i, x_str, '->', y_str)\n",
        "\n",
        "    x_data.append([char_dic[c] for c in x_str])  # x str to index\n",
        "    y_data.append([char_dic[c] for c in y_str])  # y str to index"
      ],
      "metadata": {
        "id": "6tdzu64508Bj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d146cc1c-3b92-4737-cb2c-41070286f2e1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 if you wan -> f you want\n",
            "1 f you want ->  you want \n",
            "2  you want  -> you want t\n",
            "3 you want t -> ou want to\n",
            "4 ou want to -> u want to \n",
            "5 u want to  ->  want to b\n",
            "6  want to b -> want to bu\n",
            "7 want to bu -> ant to bui\n",
            "8 ant to bui -> nt to buil\n",
            "9 nt to buil -> t to build\n",
            "10 t to build ->  to build \n",
            "11  to build  -> to build a\n",
            "12 to build a -> o build a \n",
            "13 o build a  ->  build a s\n",
            "14  build a s -> build a sh\n",
            "15 build a sh -> uild a shi\n",
            "16 uild a shi -> ild a ship\n",
            "17 ild a ship -> ld a ship,\n",
            "18 ld a ship, -> d a ship, \n",
            "19 d a ship,  ->  a ship, d\n",
            "20  a ship, d -> a ship, do\n",
            "21 a ship, do ->  ship, don\n",
            "22  ship, don -> ship, don'\n",
            "23 ship, don' -> hip, don't\n",
            "24 hip, don't -> ip, don't \n",
            "25 ip, don't  -> p, don't d\n",
            "26 p, don't d -> , don't dr\n",
            "27 , don't dr ->  don't dru\n",
            "28  don't dru -> don't drum\n",
            "29 don't drum -> on't drum \n",
            "30 on't drum  -> n't drum u\n",
            "31 n't drum u -> 't drum up\n",
            "32 't drum up -> t drum up \n",
            "33 t drum up  ->  drum up p\n",
            "34  drum up p -> drum up pe\n",
            "35 drum up pe -> rum up peo\n",
            "36 rum up peo -> um up peop\n",
            "37 um up peop -> m up peopl\n",
            "38 m up peopl ->  up people\n",
            "39  up people -> up people \n",
            "40 up people  -> p people t\n",
            "41 p people t ->  people to\n",
            "42  people to -> people tog\n",
            "43 people tog -> eople toge\n",
            "44 eople toge -> ople toget\n",
            "45 ople toget -> ple togeth\n",
            "46 ple togeth -> le togethe\n",
            "47 le togethe -> e together\n",
            "48 e together ->  together \n",
            "49  together  -> together t\n",
            "50 together t -> ogether to\n",
            "51 ogether to -> gether to \n",
            "52 gether to  -> ether to c\n",
            "53 ether to c -> ther to co\n",
            "54 ther to co -> her to col\n",
            "55 her to col -> er to coll\n",
            "56 er to coll -> r to colle\n",
            "57 r to colle ->  to collec\n",
            "58  to collec -> to collect\n",
            "59 to collect -> o collect \n",
            "60 o collect  ->  collect w\n",
            "61  collect w -> collect wo\n",
            "62 collect wo -> ollect woo\n",
            "63 ollect woo -> llect wood\n",
            "64 llect wood -> lect wood \n",
            "65 lect wood  -> ect wood a\n",
            "66 ect wood a -> ct wood an\n",
            "67 ct wood an -> t wood and\n",
            "68 t wood and ->  wood and \n",
            "69  wood and  -> wood and d\n",
            "70 wood and d -> ood and do\n",
            "71 ood and do -> od and don\n",
            "72 od and don -> d and don'\n",
            "73 d and don' ->  and don't\n",
            "74  and don't -> and don't \n",
            "75 and don't  -> nd don't a\n",
            "76 nd don't a -> d don't as\n",
            "77 d don't as ->  don't ass\n",
            "78  don't ass -> don't assi\n",
            "79 don't assi -> on't assig\n",
            "80 on't assig -> n't assign\n",
            "81 n't assign -> 't assign \n",
            "82 't assign  -> t assign t\n",
            "83 t assign t ->  assign th\n",
            "84  assign th -> assign the\n",
            "85 assign the -> ssign them\n",
            "86 ssign them -> sign them \n",
            "87 sign them  -> ign them t\n",
            "88 ign them t -> gn them ta\n",
            "89 gn them ta -> n them tas\n",
            "90 n them tas ->  them task\n",
            "91  them task -> them tasks\n",
            "92 them tasks -> hem tasks \n",
            "93 hem tasks  -> em tasks a\n",
            "94 em tasks a -> m tasks an\n",
            "95 m tasks an ->  tasks and\n",
            "96  tasks and -> tasks and \n",
            "97 tasks and  -> asks and w\n",
            "98 asks and w -> sks and wo\n",
            "99 sks and wo -> ks and wor\n",
            "100 ks and wor -> s and work\n",
            "101 s and work ->  and work,\n",
            "102  and work, -> and work, \n",
            "103 and work,  -> nd work, b\n",
            "104 nd work, b -> d work, bu\n",
            "105 d work, bu ->  work, but\n",
            "106  work, but -> work, but \n",
            "107 work, but  -> ork, but r\n",
            "108 ork, but r -> rk, but ra\n",
            "109 rk, but ra -> k, but rat\n",
            "110 k, but rat -> , but rath\n",
            "111 , but rath ->  but rathe\n",
            "112  but rathe -> but rather\n",
            "113 but rather -> ut rather \n",
            "114 ut rather  -> t rather t\n",
            "115 t rather t ->  rather te\n",
            "116  rather te -> rather tea\n",
            "117 rather tea -> ather teac\n",
            "118 ather teac -> ther teach\n",
            "119 ther teach -> her teach \n",
            "120 her teach  -> er teach t\n",
            "121 er teach t -> r teach th\n",
            "122 r teach th ->  teach the\n",
            "123  teach the -> teach them\n",
            "124 teach them -> each them \n",
            "125 each them  -> ach them t\n",
            "126 ach them t -> ch them to\n",
            "127 ch them to -> h them to \n",
            "128 h them to  ->  them to l\n",
            "129  them to l -> them to lo\n",
            "130 them to lo -> hem to lon\n",
            "131 hem to lon -> em to long\n",
            "132 em to long -> m to long \n",
            "133 m to long  ->  to long f\n",
            "134  to long f -> to long fo\n",
            "135 to long fo -> o long for\n",
            "136 o long for ->  long for \n",
            "137  long for  -> long for t\n",
            "138 long for t -> ong for th\n",
            "139 ong for th -> ng for the\n",
            "140 ng for the -> g for the \n",
            "141 g for the  ->  for the e\n",
            "142  for the e -> for the en\n",
            "143 for the en -> or the end\n",
            "144 or the end -> r the endl\n",
            "145 r the endl ->  the endle\n",
            "146  the endle -> the endles\n",
            "147 the endles -> he endless\n",
            "148 he endless -> e endless \n",
            "149 e endless  ->  endless i\n",
            "150  endless i -> endless im\n",
            "151 endless im -> ndless imm\n",
            "152 ndless imm -> dless imme\n",
            "153 dless imme -> less immen\n",
            "154 less immen -> ess immens\n",
            "155 ess immens -> ss immensi\n",
            "156 ss immensi -> s immensit\n",
            "157 s immensit ->  immensity\n",
            "158  immensity -> immensity \n",
            "159 immensity  -> mmensity o\n",
            "160 mmensity o -> mensity of\n",
            "161 mensity of -> ensity of \n",
            "162 ensity of  -> nsity of t\n",
            "163 nsity of t -> sity of th\n",
            "164 sity of th -> ity of the\n",
            "165 ity of the -> ty of the \n",
            "166 ty of the  -> y of the s\n",
            "167 y of the s ->  of the se\n",
            "168  of the se -> of the sea\n",
            "169 of the sea -> f the sea.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "총 170개의 샘플이 생성되었다.\n",
        "\n",
        "그리고 각 샘플의 각 문자들은 고유한 정수로 인코딩이 된 상태이다.\n",
        "\n",
        "첫번째 샘플의 입력 데이터와 레이블 데이터를 출력한다."
      ],
      "metadata": {
        "id": "O2vUMe400-jJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_data[0])\n",
        "print(y_data[0])"
      ],
      "metadata": {
        "id": "9ILjJ_h61FA9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c62b5e47-2990-441d-fa8a-f223bad8a2bc"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4, 13, 0, 17, 3, 14, 0, 20, 1, 6]\n",
            "[13, 0, 17, 3, 14, 0, 20, 1, 6, 23]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "한 칸씩 쉬프트 된 시퀀스가 정상적으로 출력된다.\n",
        "\n",
        "이제 입력 시퀀스에 대해서 원-핫 인코딩을 수행하고, 입력 데이터와 레이블 데이터를 텐서로 변환한다."
      ],
      "metadata": {
        "id": "g-Nhu2mC1GsO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_one_hot = [np.eye(dic_size)[x] for x in x_data] # x 데이터는 원-핫 인코딩\n",
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)"
      ],
      "metadata": {
        "id": "Aks_Iy7Q1MK-"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "훈련 데이터와 레이블 데이터의 크기를 확인한다."
      ],
      "metadata": {
        "id": "NthjbD6A1Nbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
        "print('레이블의 크기 : {}'.format(Y.shape))"
      ],
      "metadata": {
        "id": "3L6wpPxU1OyO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d9dac98-a6b2-4ac6-b34f-6c5882b5afe9"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 데이터의 크기 : torch.Size([170, 10, 25])\n",
            "레이블의 크기 : torch.Size([170, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "원-핫 인코딩 된 결과를 보기 위해서 첫번째 샘플만 출력한다."
      ],
      "metadata": {
        "id": "klxuZ-TM1Qlb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X[0])"
      ],
      "metadata": {
        "id": "9MWlPK541RCX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8aa0050b-1593-4c09-90bd-e2a19ad4eb36"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 1., 0., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "레이블 데이터의 첫번째 샘플도 출력한다."
      ],
      "metadata": {
        "id": "0I5Qc2DQ1SgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y[0])"
      ],
      "metadata": {
        "id": "lMDHTu9l1Rqr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b899041-ac6a-429f-82c1-730549ed440e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([13,  0, 17,  3, 14,  0, 20,  1,  6, 23])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "위 레이블 시퀀스는 f you want에 해당된다.\n",
        "\n",
        "이제 모델을 설계한다."
      ],
      "metadata": {
        "id": "xWpH3Eyr1U8G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2) 모델 구현하기**"
      ],
      "metadata": {
        "id": "BtNoKb_U1YEC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이번에는 은닉층을 두 개 쌓는다."
      ],
      "metadata": {
        "id": "qbsB9Jmi1aUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layers): # 현재 hidden_size는 dic_size와 같음.\n",
        "        super(Net, self).__init__()\n",
        "        self.rnn = torch.nn.RNN(input_dim, hidden_dim, num_layers=layers, batch_first=True)\n",
        "        self.fc = torch.nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _status = self.rnn(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "XMN9HEWe1Xk_"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net(dic_size, hidden_size, 2) # 이번에는 층을 두 개 쌓습니다."
      ],
      "metadata": {
        "id": "nLTds2DF1eS0"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "nn.RNN() 안에 num_layers라는 인자를 사용한다.\n",
        "\n",
        "이는 은닉층을 몇 개 쌓을 것인지를 의미한다.\n",
        "\n",
        "모델 선언 시 layers라는 인자에 2를 전달하여 은닉층을 두 개 쌓는다.\n",
        "\n",
        "비용 함수와 옵티마이저를 선언한다."
      ],
      "metadata": {
        "id": "GQ4-2_rZ1gHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), learning_rate)"
      ],
      "metadata": {
        "id": "XWpNr2Rg1lSL"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 모델에 입력을 넣어서 출력의 크기를 확인한다."
      ],
      "metadata": {
        "id": "Xg1PR8S71nLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = net(X)\n",
        "print(outputs.shape) # 3차원 텐서"
      ],
      "metadata": {
        "id": "y25hziRS1nsi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b67d6a6-308f-4086-c10f-9cebecd878fd"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([170, 10, 25])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(170, 10, 25)의 크기를 가지는데 각각 배치 차원, 시점(timesteps), 출력의 크기이다.\n",
        "\n",
        "나중에 정확도를 측정할 때는 이를 모두 펼쳐서 계산하게 되는데, 이때는 view를 사용하여 배치 차원과 시점 차원을 하나로 만든다."
      ],
      "metadata": {
        "id": "qs1yEUd71peA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs.view(-1, dic_size).shape) # 2차원 텐서로 변환."
      ],
      "metadata": {
        "id": "8e-rQ4V31sKX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1b7128e-a3db-467a-ac0c-ac7821f2cb7f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1700, 25])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "차원이 (1700, 25)가 된 것을 볼 수 있다.\n",
        "\n",
        "이제 레이블 데이터의 크기를 확인한다."
      ],
      "metadata": {
        "id": "8aaJap4q1toi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y.shape)\n",
        "print(Y.view(-1).shape)"
      ],
      "metadata": {
        "id": "sr3SV_XX1wmQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12922510-a937-41ba-eb02-6fd4f3054db6"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([170, 10])\n",
            "torch.Size([1700])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "레이블 데이터는 (170, 10)의 크기를 가지는데, 마찬가지로 나중에 정확도를 측정할 때는 이걸 펼쳐서 계산한다.\n",
        "\n",
        "이 경우 (1700)의 크기를 가지게 된다.\n",
        "\n",
        "이제 옵티마이저와 손실 함수를 정의한다."
      ],
      "metadata": {
        "id": "ZGJkpven1yHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(X) # (170, 10, 25) 크기를 가진 텐서를 매 에포크마다 모델의 입력으로 사용\n",
        "    loss = criterion(outputs.view(-1, dic_size), Y.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # results의 텐서 크기는 (170, 10)\n",
        "    results = outputs.argmax(dim=2)\n",
        "    predict_str = \"\"\n",
        "    for j, result in enumerate(results):\n",
        "        if j == 0: # 처음에는 예측 결과를 전부 가져오지만\n",
        "            predict_str += ''.join([char_set[t] for t in result])\n",
        "        else: # 그 다음에는 마지막 글자만 반복 추가\n",
        "            predict_str += char_set[result[-1]]\n",
        "\n",
        "    print(predict_str)"
      ],
      "metadata": {
        "id": "vhW3god_12J2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5449d82-4cf1-457d-b105-0d373723a47f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yyeefye,eefeefeeefeeeyeyfyey'eeyeeeeyeeeeeee'ee,fe,ffyeyeyeefe,feefyee,eeeeyefeeeyeee,effeye,feee,yfyee,efe,eyyee,efe,eeyfyeeyefye,yeee,fe,fyfyyeyeeyeefefefefey'eeyfyeee,ye,yeeffe\n",
            "                                                                                                                                                                                   \n",
            "s'e nye syuuuyukkukkkyyuuknuykyuukyyuuyyunyyynuyuukkuuykkuuyynnukynukyukuukukyuukyykukkuykukyuukyynkuukkkuykkuuukyyuyynuyunuyysukyynuykkuyukkuuuyytkyknukyaukukkuuykkkykyyyukkyyuky\n",
            "torpfmtorssssssssssssssssssssossssdsrssssssssrssssssdssssssssrs'dsrssdsssssssssssssssssssssrsss'drrssssssssssssdssssd'rrrsssssrssssrsssssssssssossssssrsssrsssdssssssssdsssssdsssss\n",
            "ttttntttsptttttttttttttttt tttttttsttttttttttfottttttstttttttotttptttttttttstttttttttttttttttttsthetttttttttsttttttttsmmttttttetttptttstttttttttttttttettttttttsltttstttsttttsstttt\n",
            "toele ettt e  te eeeeeteett eeeet teee teee  ooette et eetee ot ttoeett  ettteee ee e  eeteeeee ttoetett eee tt e  eetooeeete oeettt  tetet  teeeteettot toeetttl eett tteet tt  te\n",
            "to t o        t                                              t                                                        t    t          t                                            \n",
            "t t  dw             t t     w     t    t  t    d t        t          t    t   t   t t   t       t     t   t     t t        t     t    t       w   t          tt     tt  t  t       \n",
            "t t  two   to  o k  to      t   t    i    t    ook  k     t   o   io  o   t   to  t             t   t i   to    i   k    o     t i ko i  to   to  t        o      t  k  k  k       \n",
            "t to tt l  to to    thto   tt   t d do tt tot  t t      t to to to   to   to  to  t d  tt tt  t to   tt   to  t to  tt   t io    t  t to to   to  t  o    t   t  tt tt tto to t    \n",
            "i to t   ehth dmdd  tmdidt hto  i ehtt to dom  to   ethem to to  o o doh  to  do dt eh dm ttodo dotd  d u to dd tu  d m  o d  ht thdo do do dethi t ete e tn  d   oo d  to aodaeod \n",
            "p to m e t ao aop e t tmem  eon i eo m em too  em    toem ao eo eem  eo   eos t  t  eo ep  uoem to t  eo  eo  m eon to  em a u t toem to aelo eo  e em  u en iep  n  t  to toeme p \n",
            "p t en o e tosloi e t eodp  tos t e et tm ion  ento  toer t nto eert ton  toe t n t e e t ltoer eonrr toe eos l eot to  er toertetoer tonton  eon t eme e ens e  en  i  to eoeme p \n",
            "p t esto t to lei e t toip  tosst t et tm ie   e tos t em t  to eert to   toe tonsi t eit  toer tonsr toe tos l tot to  er toe tetoem to to   ton t ere e ens t   reit  to toemd n \n",
            "p i r to t to toiie t t ii  tosst t et tp t t lo to  t em to to lemt ton  tos tonsi to  t  toem to i  tot tor t toi t t em t   t toem to to t tor t emt   essiit  csii  to toert i \n",
            "p toesto t tontui e t siip  tosst toem tm ioos e th  t em th to  ent to s tos tonsi tosit  toem to sl todhtos t tot tom er to stethem to to l tor thertos essiit eosit  to toemtoi \n",
            "p wosstont tontui e tnsiig, tosst toem tm ioo  e th er er th to  ent toss tos tosst tosip  ther to sg dodhdos t tut eurher th stether to to l aor thertod essiip ensst  an thertoc \n",
            "p wos tont do lutie tndiigl aosst doem tp tur  enth  t er th to  ent doss tns dosst dnsiie ther thsk  tnu dos t tut duther th ct them th lo d uor therd   essiip ensit  an toemdoc \n",
            "p woentont uo lutie tndodp, aonst tout uo iarl enthr ther th lol ent don  and don't ansit lthem ths'  tnd dor t tut turher thoct them thodo d aor themdnd essitg ensit  an toerd   \n",
            "t woentont to tutie tndoip, aonst tout uo iarp enth  ther th col ect tof  tnd don't tnsit ltoem tos'  tnd tork, tut tuther thoct them toodond cor themdnd essitm ensit  an toemtol \n",
            "t woastont to lutie andiipl aonst tout uo taop e th  cher to lollect ton  an' donst dnsit ltoer tos'  tnd tork, tot tuther tooch them to dont cor toemdnd ess tm eosit  an toert i \n",
            "t toa tont do latle a diip  aorst aout up teo le th ether to lollect tons t ' donst dnsite them tos'i wnd tork, tut tather th ch them to lont dor themd d ess im ersit  an toemd l \n",
            "t tor tont da latle a dhip  aonst aout up tao le th ether to lollest too  a s donst ansite them toski w d wor o but tarher toach them to lo t aor there d ess im ersit  af themd l \n",
            "t aor tash aaocutle andhipl aonst aout up teotle thgether to lollest looi ans dosst assitn toem toski wnd work, but iarher toach them to lont for toemdnd ess im ersit  of toemwft \n",
            "t aoa bond to cutle andhipl wonst aout up teople th ether to collect boo  and dosst ansitn them tosk  wnd wor , tut iather toech them to cong for themensless ip ensig  of toemeo c\n",
            "t aoo tond to cutle andhipl wonst aout up teotle to ether to collect wood and dosst ansign toem tosk  wnd work, tut iather toech them to cong for themendless im ensigy on toemeng \n",
            "m aor iont to butle anship, aonst aoum up people th  ther to collect wood and dosst ansign them tosk  wnd work, but rathe  toech them to bong for themendless im ensigy on toemeog \n",
            "m aor iont to buile anship, aonst aoum up people to ether to lollect wood and donst ansign them tosk  and work, tut rather toach them to bong for themendless im ersigy of themeot \n",
            "mmaor iont to luile anshipy aon't aoum up people together to lollect wood and drn't ansign them tosks and work, tut ruther toach ther to bong for themendless im ersigy of therenl \n",
            "lmaoe wont ao luile a shipy don't doum up people together to collect wood and drn't d sign them tosks and work, but rather toach the  to long for themendless im ensigy of the enl \n",
            "lmaou wont ao luild a ship, don't doum up people together to chllect wood and don't d sign them tosks and dork, but rather toach them to long for themendless immersity of themenlm\n",
            "lmaou wont to build a ship, don't doum up people together to collect wood and don't d sitn them tosks and dork, but rather thach them ta bond for themendlessiimmersity of the end \n",
            "mmwou want to build a ship, don't doum up people together to bollect wood and won't dnsitn them tosks and work, but rather toech the  to long for toemendless imyensity of the end \n",
            "mmwou want to build a ship, don't drum up people togethem th bollect wood and don't dnsign them tosks and work, but rather thach them ta bong for themendless immensity of themend \n",
            "mmwou want to luild a ship, don't doum up people together te lollect wood and don't dnsign them tosks and work, but rather taach the  to lond for themendless immensigy of the eid \n",
            "mmwou want to cuild a ship, don't drum up people together te collect wood and don't dnsign them tosks and work, but rather teach the  to cong for the endless immensity of the snd \n",
            "m wou want to cuild a ship, don't drum up people together th collect wood and don't dssign them tasks and work, but rather thach them ta long for themendless immensity of the sna \n",
            "m wou want to luild anship, don't drum up people together te collect wood and don't dssign them tasks and work, but rather teach them ta long for themendless immensity of the eea \n",
            "m wou want to luild a ship, don't drum up people together to collect wood and don't dssign them tosks and work, but rather teach the  to long for the endless immensity of the s as\n",
            "m wou want to build a ship, don't drum up people together to bollect wood and don't dssign them tosks and work, but rather toach the  to long for the endless immensity of the seat\n",
            "m wou want to build a ship, don't drum up people together to collect wood and don't dssign them tosks and work, but rather teach them to long for themendless immensity of the eeam\n",
            "t wou want to build a ship, don't drum up people together te collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sean\n",
            "f tou want to build a ship, don't arum up people together te collect wood and don't assign them tasks and work, but rather teach the  to long for the endless immensity of the seao\n",
            "f wou want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the seao\n",
            "f wou want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the seam\n",
            "l wou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seat\n",
            "l aou want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the seat\n",
            "l aou want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach the  to long for the endless immensity of the seat\n",
            "l wou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "l wou want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "l wou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "l wou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea.\n",
            "l wou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the set.\n",
            "m woulwant to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the eea.\n",
            "t you want to luild a ship, don't drum up people together to lollect wood and don't assign them tasks and work, but rather toach the  ta long for tee sndless immensity of the sii.\n",
            "f you want to build a ship, don't drum up people together th collect wood and don't assign them tasks and donk, but rather thach them th bong for themerdless immensity of the eean\n",
            "f wow want to build aswhip, don't drum up people together te collect wood and won't assign them to ks and work, wat rather teach the  to cong for the endless immensity of the seac\n",
            "t woulwant to build a ship, don't arum up people together to bollect wood and won't assign them tasks and dork, but rather toach them ta long for the endless immensity of the sea.\n",
            "t you want to cuild o ship, don't drum up people together to collect wood and don't dssign them tasks and dork, but rather teach them ta long for the sndless immensigy of the sea.\n",
            "t you want to build a ship, don't arum up people together te collect wood and don't assign them tasks and dork, but rather teach them ta long for the endless immensity of the eea.\n",
            "t you want to build a ship, don't arum up people together te collect wood and don't assign them tosks and dork, but rather teach them to long for the endless immensity of the eea.\n",
            "t you want to build a ship, don't arum up people together te collect wood and don't assign them tosks and dork, but rather teach them to long for the endless immensity of the sea.\n",
            "t uou want to build a ship, don't arum up people together te collect wood and won't assign them tasks and work, but rather teach them ta bong for the endless immensity of the sea.\n",
            "t uou want to build a ship, don't arum up people together to collect wood and won't assign them tasks bnd work, but rather teach them ta long for the endless immensity of the sea.\n",
            "m eou want to butld a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them ta long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "f you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "l you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "처음에는 이상한 예측을 하지만 마지막 에포크에서는 꽤 정확한 문자을 생성하는 것을 볼 수 있다."
      ],
      "metadata": {
        "id": "lVNG6Z7u14y2"
      }
    }
  ]
}